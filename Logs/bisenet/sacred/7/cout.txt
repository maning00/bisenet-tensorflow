INFO - bisenet-v2 - Running command 'main'
INFO - bisenet-v2 - Started run with ID "7"
INFO - root - nvidia-ml-py is not installed, automatically select gpu is disabled!
INFO - root - Creating training directory: Logs/bisenet/checkpoints/bisenet-v2
WARNING:tensorflow:From train.py:107: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING - tensorflow - From train.py:107: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING - root - img_mean is not explicitly specified, using default value: None
WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.read_file is deprecated. Please use tf.io.read_file instead.

WARNING - tensorflow - From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.read_file is deprecated. Please use tf.io.read_file instead.

WARNING:tensorflow:From /home/ubuntu/bisenet-tensorflow-master/Dataset/dataset.py:102: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING - tensorflow - From /home/ubuntu/bisenet-tensorflow-master/Dataset/dataset.py:102: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
INFO - root - preproces -- augment
WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING - tensorflow - From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING - tensorflow - From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
/home/ubuntu/bisenet-tensorflow-master/Dataset/dataset.py:220: UserWarning: Seed 123 from outer graph might be getting used by function Dataset_map__image_mirroring, if the random op has not been provided any seed. Explicitly set the seed in the function if this is not the intended behavior.
  dataset = dataset.map(_image_mirroring, num_parallel_calls=threads)
WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.

WARNING - tensorflow - From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.

WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.

WARNING - tensorflow - From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.

WARNING:tensorflow:From /home/ubuntu/bisenet-tensorflow-master/Dataset/dataset.py:121: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING - tensorflow - From /home/ubuntu/bisenet-tensorflow-master/Dataset/dataset.py:121: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
/home/ubuntu/bisenet-tensorflow-master/Dataset/dataset.py:222: UserWarning: Seed 123 from outer graph might be getting used by function Dataset_map__image_scaling, if the random op has not been provided any seed. Explicitly set the seed in the function if this is not the intended behavior.
  dataset = dataset.map(_image_scaling, num_parallel_calls=threads)
WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.

WARNING - tensorflow - From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.

/home/ubuntu/bisenet-tensorflow-master/Dataset/dataset.py:225: UserWarning: Seed 123 from outer graph might be getting used by function Dataset_map_<lambda>, if the random op has not been provided any seed. Explicitly set the seed in the function if this is not the intended behavior.
  num_parallel_calls=threads)
/home/ubuntu/bisenet-tensorflow-master/Dataset/dataset.py:226: UserWarning: Seed 123 from outer graph might be getting used by function Dataset_map_<lambda>, if the random op has not been provided any seed. Explicitly set the seed in the function if this is not the intended behavior.
  dataset = dataset.map(lambda image, label: _apply_with_random_selector(image, lambda x, ordering: _distort_color
WARNING:tensorflow:From /home/ubuntu/bisenet-tensorflow-master/Dataset/dataset.py:237: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.
WARNING - tensorflow - From /home/ubuntu/bisenet-tensorflow-master/Dataset/dataset.py:237: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.
WARNING:tensorflow:From /home/ubuntu/bisenet-tensorflow-master/models/bisenet.py:182: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING - tensorflow - From /home/ubuntu/bisenet-tensorflow-master/models/bisenet.py:182: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING - tensorflow - From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From /home/ubuntu/bisenet-tensorflow-master/models/bisenet.py:204: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.

WARNING - tensorflow - From /home/ubuntu/bisenet-tensorflow-master/models/bisenet.py:204: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.

WARNING:tensorflow:From /home/ubuntu/bisenet-tensorflow-master/models/bisenet.py:242: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

WARNING - tensorflow - From /home/ubuntu/bisenet-tensorflow-master/models/bisenet.py:242: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

WARNING:tensorflow:From /home/ubuntu/bisenet-tensorflow-master/models/bisenet.py:246: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.

WARNING - tensorflow - From /home/ubuntu/bisenet-tensorflow-master/models/bisenet.py:246: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.

WARNING:tensorflow:From /home/ubuntu/bisenet-tensorflow-master/models/bisenet.py:249: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.

WARNING - tensorflow - From /home/ubuntu/bisenet-tensorflow-master/models/bisenet.py:249: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.

WARNING:tensorflow:From /home/ubuntu/bisenet-tensorflow-master/models/bisenet.py:254: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.

WARNING - tensorflow - From /home/ubuntu/bisenet-tensorflow-master/models/bisenet.py:254: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.

WARNING:tensorflow:From /home/ubuntu/bisenet-tensorflow-master/models/bisenet.py:261: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING - tensorflow - From /home/ubuntu/bisenet-tensorflow-master/models/bisenet.py:261: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/ubuntu/bisenet-tensorflow-master/models/bisenet.py:265: streaming_accuracy (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.metrics.accuracy. Note that the order of the labels and predictions arguments has been switched.
WARNING - tensorflow - From /home/ubuntu/bisenet-tensorflow-master/models/bisenet.py:265: streaming_accuracy (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.metrics.accuracy. Note that the order of the labels and predictions arguments has been switched.
WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/metrics_impl.py:1178: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING - tensorflow - From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/metrics_impl.py:1178: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING - root - img_mean is not explicitly specified, using default value: None
WARNING - root - random_scale is not explicitly specified, using default value: False
WARNING - root - random_mirror is not explicitly specified, using default value: True
INFO - root - preproces -- None
WARNING:tensorflow:From train.py:78: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

WARNING - tensorflow - From train.py:78: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

WARNING:tensorflow:From train.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.

WARNING - tensorflow - From train.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.

WARNING:tensorflow:From train.py:133: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING - tensorflow - From train.py:133: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING:tensorflow:From train.py:133: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING - tensorflow - From train.py:133: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From train.py:136: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

WARNING - tensorflow - From train.py:136: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

WARNING:tensorflow:From train.py:137: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING - tensorflow - From train.py:137: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING:tensorflow:From train.py:139: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

WARNING - tensorflow - From train.py:139: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

WARNING:tensorflow:From train.py:140: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.

WARNING - tensorflow - From train.py:140: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.

WARNING:tensorflow:From train.py:144: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

WARNING - tensorflow - From train.py:144: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

WARNING:tensorflow:From train.py:145: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING - tensorflow - From train.py:145: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From train.py:147: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING - tensorflow - From train.py:147: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

INFO - root - Train for 2500 steps
INFO - root - 2020-04-09 15:34:01.437395: step 0, total loss = 13.05, predict loss = 5.07 (0.2 examples/sec; 22.780 sec/batch; 15h:49m:10s remains)
INFO - root - 2020-04-09 15:34:14.166048: step 10, total loss = 4.29, predict loss = 2.17 (5.6 examples/sec; 0.719 sec/batch; 0h:29m:49s remains)
INFO - root - 2020-04-09 15:34:22.042442: step 20, total loss = 0.73, predict loss = 0.10 (5.5 examples/sec; 0.722 sec/batch; 0h:29m:50s remains)
INFO - root - 2020-04-09 15:34:40.359813: step 30, total loss = 1.01, predict loss = 0.20 (5.5 examples/sec; 0.733 sec/batch; 0h:30m:11s remains)
INFO - root - 2020-04-09 15:34:48.141205: step 40, total loss = 1.28, predict loss = 0.10 (5.6 examples/sec; 0.720 sec/batch; 0h:29m:30s remains)
INFO - root - 2020-04-09 15:35:06.258426: step 50, total loss = 2.77, predict loss = 0.88 (5.6 examples/sec; 0.718 sec/batch; 0h:29m:20s remains)
INFO - root - 2020-04-09 15:35:15.093538: step 60, total loss = 0.53, predict loss = 0.08 (5.3 examples/sec; 0.749 sec/batch; 0h:30m:28s remains)
INFO - root - 2020-04-09 15:35:33.598000: step 70, total loss = 0.56, predict loss = 0.06 (5.4 examples/sec; 0.735 sec/batch; 0h:29m:46s remains)
INFO - root - 2020-04-09 15:35:41.385034: step 80, total loss = 1.15, predict loss = 0.23 (5.6 examples/sec; 0.720 sec/batch; 0h:29m:01s remains)
INFO - root - 2020-04-09 15:35:49.167618: step 90, total loss = 2.73, predict loss = 0.46 (5.6 examples/sec; 0.720 sec/batch; 0h:28m:54s remains)
INFO - root - 2020-04-09 15:36:08.320514: step 100, total loss = 4.09, predict loss = 1.68 (5.5 examples/sec; 0.722 sec/batch; 0h:28m:51s remains)
INFO - root - 2020-04-09 15:36:17.122261: step 110, total loss = 0.87, predict loss = 0.18 (5.5 examples/sec; 0.729 sec/batch; 0h:29m:02s remains)
INFO - root - 2020-04-09 15:36:34.701872: step 120, total loss = 0.68, predict loss = 0.09 (5.5 examples/sec; 0.721 sec/batch; 0h:28m:36s remains)
INFO - root - 2020-04-09 15:36:42.474204: step 130, total loss = 2.54, predict loss = 0.39 (5.5 examples/sec; 0.734 sec/batch; 0h:28m:59s remains)
INFO - root - 2020-04-09 15:37:00.859104: step 140, total loss = 1.05, predict loss = 0.12 (5.3 examples/sec; 0.749 sec/batch; 0h:29m:27s remains)
INFO - root - 2020-04-09 15:37:08.645940: step 150, total loss = 1.71, predict loss = 0.39 (5.6 examples/sec; 0.720 sec/batch; 0h:28m:12s remains)
INFO - root - 2020-04-09 15:37:27.450436: step 160, total loss = 1.54, predict loss = 0.38 (5.5 examples/sec; 0.722 sec/batch; 0h:28m:10s remains)
INFO - root - 2020-04-09 15:37:35.206435: step 170, total loss = 3.64, predict loss = 1.22 (5.5 examples/sec; 0.729 sec/batch; 0h:28m:18s remains)
INFO - root - 2020-04-09 15:37:42.973872: step 180, total loss = 1.72, predict loss = 0.71 (5.5 examples/sec; 0.723 sec/batch; 0h:27m:56s remains)
INFO - root - 2020-04-09 15:38:01.170884: step 190, total loss = 0.77, predict loss = 0.12 (5.6 examples/sec; 0.721 sec/batch; 0h:27m:44s remains)
INFO - root - 2020-04-09 15:38:08.954535: step 200, total loss = 0.51, predict loss = 0.06 (5.5 examples/sec; 0.730 sec/batch; 0h:27m:59s remains)
INFO - root - 2020-04-09 15:38:28.188373: step 210, total loss = 0.99, predict loss = 0.03 (5.4 examples/sec; 0.738 sec/batch; 0h:28m:09s remains)
INFO - root - 2020-04-09 15:38:35.958615: step 220, total loss = 0.55, predict loss = 0.08 (5.5 examples/sec; 0.722 sec/batch; 0h:27m:26s remains)
INFO - root - 2020-04-09 15:38:54.298233: step 230, total loss = 2.56, predict loss = 0.32 (5.6 examples/sec; 0.721 sec/batch; 0h:27m:15s remains)
INFO - root - 2020-04-09 15:39:02.063110: step 240, total loss = 0.97, predict loss = 0.16 (5.6 examples/sec; 0.720 sec/batch; 0h:27m:06s remains)
INFO - root - 2020-04-09 15:39:20.101314: step 250, total loss = 2.27, predict loss = 0.43 (0.4 examples/sec; 11.009 sec/batch; 6h:52m:50s remains)
INFO - root - 2020-04-09 15:39:28.032675: step 260, total loss = 0.71, predict loss = 0.13 (5.6 examples/sec; 0.719 sec/batch; 0h:26m:50s remains)
INFO - root - 2020-04-09 15:39:36.681394: step 270, total loss = 0.47, predict loss = 0.04 (5.5 examples/sec; 0.730 sec/batch; 0h:27m:08s remains)
INFO - root - 2020-04-09 15:39:54.852554: step 280, total loss = 0.80, predict loss = 0.11 (5.6 examples/sec; 0.719 sec/batch; 0h:26m:37s remains)
INFO - root - 2020-04-09 15:40:02.623557: step 290, total loss = 0.16, predict loss = 0.01 (5.5 examples/sec; 0.721 sec/batch; 0h:26m:33s remains)
INFO - root - 2020-04-09 15:40:20.678954: step 300, total loss = 0.87, predict loss = 0.19 (5.5 examples/sec; 0.722 sec/batch; 0h:26m:27s remains)
INFO - root - 2020-04-09 15:40:28.548215: step 310, total loss = 0.44, predict loss = 0.07 (5.5 examples/sec; 0.725 sec/batch; 0h:26m:27s remains)
INFO - root - 2020-04-09 15:40:48.126649: step 320, total loss = 0.17, predict loss = 0.01 (5.6 examples/sec; 0.719 sec/batch; 0h:26m:06s remains)
INFO - root - 2020-04-09 15:40:55.898368: step 330, total loss = 0.61, predict loss = 0.07 (5.6 examples/sec; 0.719 sec/batch; 0h:26m:00s remains)
INFO - root - 2020-04-09 15:41:03.691964: step 340, total loss = 1.15, predict loss = 0.33 (5.4 examples/sec; 0.741 sec/batch; 0h:26m:39s remains)
INFO - root - 2020-04-09 15:41:22.117750: step 350, total loss = 0.55, predict loss = 0.11 (5.6 examples/sec; 0.720 sec/batch; 0h:25m:48s remains)
INFO - root - 2020-04-09 15:41:29.977673: step 360, total loss = 0.51, predict loss = 0.05 (5.6 examples/sec; 0.718 sec/batch; 0h:25m:37s remains)
INFO - root - 2020-04-09 15:41:49.218202: step 370, total loss = 0.52, predict loss = 0.10 (5.5 examples/sec; 0.732 sec/batch; 0h:25m:59s remains)
INFO - root - 2020-04-09 15:41:56.983346: step 380, total loss = 1.09, predict loss = 0.35 (5.6 examples/sec; 0.721 sec/batch; 0h:25m:27s remains)
INFO - root - 2020-04-09 15:42:16.157011: step 390, total loss = 0.41, predict loss = 0.14 (5.6 examples/sec; 0.719 sec/batch; 0h:25m:17s remains)
INFO - root - 2020-04-09 15:42:23.939498: step 400, total loss = 1.22, predict loss = 0.47 (5.6 examples/sec; 0.721 sec/batch; 0h:25m:13s remains)
INFO - root - 2020-04-09 15:42:42.621615: step 410, total loss = 0.58, predict loss = 0.14 (5.4 examples/sec; 0.740 sec/batch; 0h:25m:45s remains)
INFO - root - 2020-04-09 15:42:51.269715: step 420, total loss = 0.89, predict loss = 0.22 (5.6 examples/sec; 0.720 sec/batch; 0h:24m:56s remains)
INFO - root - 2020-04-09 15:42:59.022046: step 430, total loss = 0.51, predict loss = 0.23 (5.5 examples/sec; 0.727 sec/batch; 0h:25m:04s remains)
INFO - root - 2020-04-09 15:43:17.999951: step 440, total loss = 0.64, predict loss = 0.08 (5.3 examples/sec; 0.751 sec/batch; 0h:25m:46s remains)
INFO - root - 2020-04-09 15:43:25.780393: step 450, total loss = 0.67, predict loss = 0.05 (5.5 examples/sec; 0.722 sec/batch; 0h:24m:40s remains)
INFO - root - 2020-04-09 15:43:43.899637: step 460, total loss = 0.46, predict loss = 0.09 (5.6 examples/sec; 0.719 sec/batch; 0h:24m:27s remains)
INFO - root - 2020-04-09 15:43:52.522996: step 470, total loss = 0.15, predict loss = 0.04 (5.6 examples/sec; 0.720 sec/batch; 0h:24m:21s remains)
INFO - root - 2020-04-09 15:44:10.082958: step 480, total loss = 0.44, predict loss = 0.04 (5.6 examples/sec; 0.718 sec/batch; 0h:24m:10s remains)
INFO - root - 2020-04-09 15:44:17.866415: step 490, total loss = 0.80, predict loss = 0.08 (5.5 examples/sec; 0.731 sec/batch; 0h:24m:30s remains)
INFO - root - 2020-04-09 15:44:36.252589: step 500, total loss = 0.87, predict loss = 0.32 (0.4 examples/sec; 11.358 sec/batch; 6h:18m:36s remains)
INFO - root - 2020-04-09 15:44:44.169944: step 510, total loss = 0.75, predict loss = 0.12 (5.6 examples/sec; 0.719 sec/batch; 0h:23m:51s remains)
INFO - root - 2020-04-09 15:44:51.912025: step 520, total loss = 0.56, predict loss = 0.05 (5.6 examples/sec; 0.718 sec/batch; 0h:23m:42s remains)
INFO - root - 2020-04-09 15:45:10.904752: step 530, total loss = 0.52, predict loss = 0.09 (5.6 examples/sec; 0.719 sec/batch; 0h:23m:35s remains)
INFO - root - 2020-04-09 15:45:18.679201: step 540, total loss = 0.48, predict loss = 0.04 (5.5 examples/sec; 0.723 sec/batch; 0h:23m:37s remains)
INFO - root - 2020-04-09 15:45:37.137793: step 550, total loss = 0.18, predict loss = 0.03 (5.5 examples/sec; 0.732 sec/batch; 0h:23m:47s remains)
INFO - root - 2020-04-09 15:45:45.048384: step 560, total loss = 2.78, predict loss = 0.64 (5.5 examples/sec; 0.732 sec/batch; 0h:23m:39s remains)
INFO - root - 2020-04-09 15:46:03.605263: step 570, total loss = 0.36, predict loss = 0.07 (5.6 examples/sec; 0.720 sec/batch; 0h:23m:09s remains)
INFO - root - 2020-04-09 15:46:12.238263: step 580, total loss = 0.37, predict loss = 0.10 (5.5 examples/sec; 0.730 sec/batch; 0h:23m:21s remains)
INFO - root - 2020-04-09 15:46:20.018592: step 590, total loss = 0.19, predict loss = 0.02 (5.6 examples/sec; 0.721 sec/batch; 0h:22m:56s remains)
INFO - root - 2020-04-09 15:46:38.596197: step 600, total loss = 0.12, predict loss = 0.01 (5.6 examples/sec; 0.719 sec/batch; 0h:22m:45s remains)
INFO - root - 2020-04-09 15:46:46.521143: step 610, total loss = 0.60, predict loss = 0.13 (5.4 examples/sec; 0.735 sec/batch; 0h:23m:08s remains)
INFO - root - 2020-04-09 15:47:04.717495: step 620, total loss = 0.42, predict loss = 0.04 (5.5 examples/sec; 0.732 sec/batch; 0h:22m:55s remains)
INFO - root - 2020-04-09 15:47:13.342803: step 630, total loss = 0.68, predict loss = 0.03 (5.5 examples/sec; 0.732 sec/batch; 0h:22m:48s remains)
INFO - root - 2020-04-09 15:47:31.153798: step 640, total loss = 0.37, predict loss = 0.06 (5.6 examples/sec; 0.720 sec/batch; 0h:22m:18s remains)
INFO - root - 2020-04-09 15:47:38.938091: step 650, total loss = 0.33, predict loss = 0.06 (5.5 examples/sec; 0.731 sec/batch; 0h:22m:32s remains)
INFO - root - 2020-04-09 15:47:57.771574: step 660, total loss = 0.21, predict loss = 0.02 (5.5 examples/sec; 0.721 sec/batch; 0h:22m:07s remains)
INFO - root - 2020-04-09 15:48:05.515437: step 670, total loss = 0.81, predict loss = 0.33 (5.6 examples/sec; 0.720 sec/batch; 0h:21m:57s remains)
INFO - root - 2020-04-09 15:48:14.148152: step 680, total loss = 0.28, predict loss = 0.04 (5.6 examples/sec; 0.719 sec/batch; 0h:21m:48s remains)
INFO - root - 2020-04-09 15:48:32.418334: step 690, total loss = 0.51, predict loss = 0.06 (5.5 examples/sec; 0.733 sec/batch; 0h:22m:07s remains)
INFO - root - 2020-04-09 15:48:40.164370: step 700, total loss = 0.31, predict loss = 0.05 (5.6 examples/sec; 0.720 sec/batch; 0h:21m:36s remains)
INFO - root - 2020-04-09 15:48:59.435365: step 710, total loss = 0.90, predict loss = 0.18 (5.6 examples/sec; 0.718 sec/batch; 0h:21m:25s remains)
INFO - root - 2020-04-09 15:49:07.240864: step 720, total loss = 0.12, predict loss = 0.02 (5.6 examples/sec; 0.720 sec/batch; 0h:21m:20s remains)
INFO - root - 2020-04-09 15:49:26.336745: step 730, total loss = 0.42, predict loss = 0.04 (5.6 examples/sec; 0.720 sec/batch; 0h:21m:13s remains)
INFO - root - 2020-04-09 15:49:34.083663: step 740, total loss = 0.26, predict loss = 0.02 (5.5 examples/sec; 0.722 sec/batch; 0h:21m:10s remains)
INFO - root - 2020-04-09 15:49:51.898945: step 750, total loss = 0.54, predict loss = 0.09 (0.4 examples/sec; 10.797 sec/batch; 5h:14m:55s remains)
INFO - root - 2020-04-09 15:49:59.805885: step 760, total loss = 0.30, predict loss = 0.06 (5.5 examples/sec; 0.733 sec/batch; 0h:21m:16s remains)
INFO - root - 2020-04-09 15:50:07.571325: step 770, total loss = 2.36, predict loss = 0.45 (5.5 examples/sec; 0.724 sec/batch; 0h:20m:51s remains)
INFO - root - 2020-04-09 15:50:26.899731: step 780, total loss = 0.41, predict loss = 0.09 (5.6 examples/sec; 0.718 sec/batch; 0h:20m:35s remains)
INFO - root - 2020-04-09 15:50:35.543734: step 790, total loss = 0.19, predict loss = 0.03 (5.5 examples/sec; 0.732 sec/batch; 0h:20m:52s remains)
INFO - root - 2020-04-09 15:50:53.626630: step 800, total loss = 0.27, predict loss = 0.07 (5.6 examples/sec; 0.719 sec/batch; 0h:20m:22s remains)
INFO - root - 2020-04-09 15:51:01.559535: step 810, total loss = 0.37, predict loss = 0.10 (5.5 examples/sec; 0.730 sec/batch; 0h:20m:34s remains)
INFO - root - 2020-04-09 15:51:20.619892: step 820, total loss = 0.29, predict loss = 0.05 (5.6 examples/sec; 0.720 sec/batch; 0h:20m:09s remains)
INFO - root - 2020-04-09 15:51:28.373632: step 830, total loss = 0.89, predict loss = 0.15 (5.5 examples/sec; 0.731 sec/batch; 0h:20m:20s remains)
INFO - root - 2020-04-09 15:51:37.032946: step 840, total loss = 1.41, predict loss = 0.50 (5.6 examples/sec; 0.720 sec/batch; 0h:19m:55s remains)
INFO - root - 2020-04-09 15:51:54.788520: step 850, total loss = 0.39, predict loss = 0.03 (5.6 examples/sec; 0.720 sec/batch; 0h:19m:48s remains)
INFO - root - 2020-04-09 15:52:02.680808: step 860, total loss = 0.78, predict loss = 0.10 (5.6 examples/sec; 0.720 sec/batch; 0h:19m:41s remains)
INFO - root - 2020-04-09 15:52:21.200552: step 870, total loss = 0.64, predict loss = 0.11 (5.5 examples/sec; 0.728 sec/batch; 0h:19m:47s remains)
INFO - root - 2020-04-09 15:52:28.989816: step 880, total loss = 0.53, predict loss = 0.08 (5.5 examples/sec; 0.733 sec/batch; 0h:19m:47s remains)
INFO - root - 2020-04-09 15:52:47.553724: step 890, total loss = 1.08, predict loss = 0.05 (5.5 examples/sec; 0.729 sec/batch; 0h:19m:34s remains)
INFO - root - 2020-04-09 15:52:55.297259: step 900, total loss = 0.56, predict loss = 0.06 (5.5 examples/sec; 0.721 sec/batch; 0h:19m:14s remains)
INFO - root - 2020-04-09 15:53:13.979769: step 910, total loss = 0.76, predict loss = 0.09 (5.5 examples/sec; 0.728 sec/batch; 0h:19m:17s remains)
INFO - root - 2020-04-09 15:53:21.722773: step 920, total loss = 0.44, predict loss = 0.11 (5.5 examples/sec; 0.721 sec/batch; 0h:18m:58s remains)
INFO - root - 2020-04-09 15:53:29.473404: step 930, total loss = 0.37, predict loss = 0.12 (5.5 examples/sec; 0.721 sec/batch; 0h:18m:51s remains)
INFO - root - 2020-04-09 15:53:48.654375: step 940, total loss = 0.29, predict loss = 0.02 (5.5 examples/sec; 0.728 sec/batch; 0h:18m:56s remains)
INFO - root - 2020-04-09 15:53:56.457770: step 950, total loss = 0.30, predict loss = 0.03 (5.4 examples/sec; 0.743 sec/batch; 0h:19m:11s remains)
INFO - root - 2020-04-09 15:54:15.112491: step 960, total loss = 0.13, predict loss = 0.05 (5.6 examples/sec; 0.718 sec/batch; 0h:18m:25s remains)
INFO - root - 2020-04-09 15:54:22.877224: step 970, total loss = 0.46, predict loss = 0.11 (5.6 examples/sec; 0.719 sec/batch; 0h:18m:19s remains)
INFO - root - 2020-04-09 15:54:41.023371: step 980, total loss = 0.39, predict loss = 0.06 (5.6 examples/sec; 0.719 sec/batch; 0h:18m:13s remains)
INFO - root - 2020-04-09 15:54:49.666829: step 990, total loss = 0.60, predict loss = 0.13 (5.5 examples/sec; 0.721 sec/batch; 0h:18m:09s remains)
INFO - root - 2020-04-09 15:55:08.506787: step 1000, total loss = 0.23, predict loss = 0.07 (0.3 examples/sec; 11.807 sec/batch; 4h:55m:10s remains)
INFO - root - 2020-04-09 15:55:16.394934: step 1010, total loss = 0.15, predict loss = 0.03 (5.6 examples/sec; 0.720 sec/batch; 0h:17m:53s remains)
INFO - root - 2020-04-09 15:55:24.129294: step 1020, total loss = 0.13, predict loss = 0.02 (5.6 examples/sec; 0.720 sec/batch; 0h:17m:45s remains)
INFO - root - 2020-04-09 15:55:41.877096: step 1030, total loss = 0.19, predict loss = 0.02 (5.6 examples/sec; 0.719 sec/batch; 0h:17m:36s remains)
INFO - root - 2020-04-09 15:55:49.635055: step 1040, total loss = 0.96, predict loss = 0.08 (5.5 examples/sec; 0.721 sec/batch; 0h:17m:32s remains)
WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py:969: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
WARNING - tensorflow - From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py:969: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
INFO - root - 2020-04-09 15:56:09.508096: step 1050, total loss = 0.55, predict loss = 0.08 (5.5 examples/sec; 0.730 sec/batch; 0h:17m:37s remains)
INFO - root - 2020-04-09 15:56:17.447725: step 1060, total loss = 0.17, predict loss = 0.02 (5.5 examples/sec; 0.732 sec/batch; 0h:17m:34s remains)
INFO - root - 2020-04-09 15:56:35.578182: step 1070, total loss = 0.35, predict loss = 0.06 (5.6 examples/sec; 0.719 sec/batch; 0h:17m:08s remains)
INFO - root - 2020-04-09 15:56:43.354465: step 1080, total loss = 0.31, predict loss = 0.04 (5.6 examples/sec; 0.720 sec/batch; 0h:17m:01s remains)
INFO - root - 2020-04-09 15:56:51.134074: step 1090, total loss = 1.21, predict loss = 0.16 (5.5 examples/sec; 0.723 sec/batch; 0h:16m:59s remains)
INFO - root - 2020-04-09 15:57:10.734701: step 1100, total loss = 0.28, predict loss = 0.05 (5.5 examples/sec; 0.731 sec/batch; 0h:17m:03s remains)
INFO - root - 2020-04-09 15:57:18.664587: step 1110, total loss = 0.24, predict loss = 0.02 (5.6 examples/sec; 0.718 sec/batch; 0h:16m:37s remains)
INFO - root - 2020-04-09 15:57:36.990766: step 1120, total loss = 0.17, predict loss = 0.01 (5.5 examples/sec; 0.730 sec/batch; 0h:16m:47s remains)
INFO - root - 2020-04-09 15:57:44.736953: step 1130, total loss = 0.15, predict loss = 0.02 (5.5 examples/sec; 0.721 sec/batch; 0h:16m:28s remains)
INFO - root - 2020-04-09 15:58:01.866722: step 1140, total loss = 0.22, predict loss = 0.03 (5.5 examples/sec; 0.730 sec/batch; 0h:16m:33s remains)
INFO - root - 2020-04-09 15:58:10.542009: step 1150, total loss = 3.05, predict loss = 1.07 (5.5 examples/sec; 0.734 sec/batch; 0h:16m:30s remains)
INFO - root - 2020-04-09 15:58:29.479150: step 1160, total loss = 0.26, predict loss = 0.06 (5.6 examples/sec; 0.719 sec/batch; 0h:16m:03s remains)
INFO - root - 2020-04-09 15:58:37.247223: step 1170, total loss = 0.31, predict loss = 0.04 (5.6 examples/sec; 0.720 sec/batch; 0h:15m:57s remains)
INFO - root - 2020-04-09 15:58:45.002104: step 1180, total loss = 0.29, predict loss = 0.05 (5.5 examples/sec; 0.727 sec/batch; 0h:15m:59s remains)
INFO - root - 2020-04-09 15:59:04.095033: step 1190, total loss = 0.06, predict loss = 0.01 (5.5 examples/sec; 0.728 sec/batch; 0h:15m:53s remains)
INFO - root - 2020-04-09 15:59:12.754432: step 1200, total loss = 1.11, predict loss = 0.09 (5.6 examples/sec; 0.719 sec/batch; 0h:15m:35s remains)
INFO - root - 2020-04-09 15:59:30.958946: step 1210, total loss = 0.81, predict loss = 0.07 (5.6 examples/sec; 0.720 sec/batch; 0h:15m:28s remains)
INFO - root - 2020-04-09 15:59:38.751580: step 1220, total loss = 0.31, predict loss = 0.06 (5.4 examples/sec; 0.743 sec/batch; 0h:15m:50s remains)
INFO - root - 2020-04-09 15:59:57.285307: step 1230, total loss = 0.26, predict loss = 0.03 (5.5 examples/sec; 0.729 sec/batch; 0h:15m:25s remains)
INFO - root - 2020-04-09 16:00:05.062396: step 1240, total loss = 0.23, predict loss = 0.01 (5.5 examples/sec; 0.733 sec/batch; 0h:15m:23s remains)
INFO - root - 2020-04-09 16:00:23.644293: step 1250, total loss = 0.24, predict loss = 0.02 (0.4 examples/sec; 10.615 sec/batch; 3h:41m:09s remains)
INFO - root - 2020-04-09 16:00:31.545612: step 1260, total loss = 0.40, predict loss = 0.02 (5.4 examples/sec; 0.735 sec/batch; 0h:15m:11s remains)
INFO - root - 2020-04-09 16:00:39.314824: step 1270, total loss = 0.44, predict loss = 0.05 (5.6 examples/sec; 0.720 sec/batch; 0h:14m:45s remains)
INFO - root - 2020-04-09 16:00:56.942605: step 1280, total loss = 0.18, predict loss = 0.01 (5.5 examples/sec; 0.722 sec/batch; 0h:14m:40s remains)
INFO - root - 2020-04-09 16:01:04.667005: step 1290, total loss = 0.09, predict loss = 0.01 (5.5 examples/sec; 0.721 sec/batch; 0h:14m:32s remains)
INFO - root - 2020-04-09 16:01:23.145498: step 1300, total loss = 0.34, predict loss = 0.08 (5.5 examples/sec; 0.722 sec/batch; 0h:14m:26s remains)
INFO - root - 2020-04-09 16:01:31.945137: step 1310, total loss = 0.21, predict loss = 0.01 (5.5 examples/sec; 0.722 sec/batch; 0h:14m:19s remains)
INFO - root - 2020-04-09 16:01:51.149602: step 1320, total loss = 0.25, predict loss = 0.01 (5.5 examples/sec; 0.728 sec/batch; 0h:14m:19s remains)
INFO - root - 2020-04-09 16:01:58.908178: step 1330, total loss = 0.52, predict loss = 0.39 (5.5 examples/sec; 0.723 sec/batch; 0h:14m:05s remains)
INFO - root - 2020-04-09 16:02:06.691091: step 1340, total loss = 0.73, predict loss = 0.12 (5.6 examples/sec; 0.718 sec/batch; 0h:13m:53s remains)
INFO - root - 2020-04-09 16:02:24.379571: step 1350, total loss = 0.25, predict loss = 0.02 (5.6 examples/sec; 0.719 sec/batch; 0h:13m:46s remains)
INFO - root - 2020-04-09 16:02:33.144524: step 1360, total loss = 0.32, predict loss = 0.05 (5.5 examples/sec; 0.732 sec/batch; 0h:13m:54s remains)
INFO - root - 2020-04-09 16:02:52.235657: step 1370, total loss = 0.31, predict loss = 0.05 (5.6 examples/sec; 0.719 sec/batch; 0h:13m:32s remains)
INFO - root - 2020-04-09 16:02:59.970958: step 1380, total loss = 0.26, predict loss = 0.05 (5.5 examples/sec; 0.724 sec/batch; 0h:13m:30s remains)
INFO - root - 2020-04-09 16:03:18.909921: step 1390, total loss = 0.27, predict loss = 0.02 (5.6 examples/sec; 0.719 sec/batch; 0h:13m:18s remains)
INFO - root - 2020-04-09 16:03:26.690145: step 1400, total loss = 0.73, predict loss = 0.03 (5.4 examples/sec; 0.745 sec/batch; 0h:13m:39s remains)
INFO - root - 2020-04-09 16:03:45.683847: step 1410, total loss = 0.54, predict loss = 0.14 (5.4 examples/sec; 0.740 sec/batch; 0h:13m:26s remains)
INFO - root - 2020-04-09 16:03:53.475204: step 1420, total loss = 0.36, predict loss = 0.02 (5.6 examples/sec; 0.720 sec/batch; 0h:12m:57s remains)
INFO - root - 2020-04-09 16:04:01.198574: step 1430, total loss = 0.44, predict loss = 0.11 (5.6 examples/sec; 0.720 sec/batch; 0h:12m:50s remains)
INFO - root - 2020-04-09 16:04:20.657057: step 1440, total loss = 0.38, predict loss = 0.06 (5.6 examples/sec; 0.719 sec/batch; 0h:12m:42s remains)
INFO - root - 2020-04-09 16:04:28.481942: step 1450, total loss = 0.32, predict loss = 0.04 (5.6 examples/sec; 0.719 sec/batch; 0h:12m:34s remains)
INFO - root - 2020-04-09 16:04:47.628111: step 1460, total loss = 0.43, predict loss = 0.04 (5.5 examples/sec; 0.733 sec/batch; 0h:12m:42s remains)
INFO - root - 2020-04-09 16:04:55.381748: step 1470, total loss = 0.15, predict loss = 0.01 (5.6 examples/sec; 0.720 sec/batch; 0h:12m:21s remains)
INFO - root - 2020-04-09 16:05:12.860485: step 1480, total loss = 0.29, predict loss = 0.06 (5.4 examples/sec; 0.747 sec/batch; 0h:12m:41s remains)
INFO - root - 2020-04-09 16:05:20.617968: step 1490, total loss = 0.20, predict loss = 0.03 (5.4 examples/sec; 0.736 sec/batch; 0h:12m:23s remains)
INFO - root - 2020-04-09 16:05:38.785813: step 1500, total loss = 0.15, predict loss = 0.01 (0.4 examples/sec; 11.131 sec/batch; 3h:05m:31s remains)
INFO - root - 2020-04-09 16:05:47.605414: step 1510, total loss = 0.13, predict loss = 0.00 (5.4 examples/sec; 0.735 sec/batch; 0h:12m:07s remains)
INFO - root - 2020-04-09 16:05:55.357543: step 1520, total loss = 0.26, predict loss = 0.03 (5.6 examples/sec; 0.718 sec/batch; 0h:11m:43s remains)
INFO - root - 2020-04-09 16:06:13.738363: step 1530, total loss = 0.08, predict loss = 0.00 (5.6 examples/sec; 0.719 sec/batch; 0h:11m:37s remains)
INFO - root - 2020-04-09 16:06:21.482091: step 1540, total loss = 0.21, predict loss = 0.01 (5.6 examples/sec; 0.720 sec/batch; 0h:11m:30s remains)
INFO - root - 2020-04-09 16:06:39.658498: step 1550, total loss = 0.59, predict loss = 0.06 (5.5 examples/sec; 0.721 sec/batch; 0h:11m:24s remains)
INFO - root - 2020-04-09 16:06:47.587026: step 1560, total loss = 1.06, predict loss = 0.26 (5.5 examples/sec; 0.729 sec/batch; 0h:11m:24s remains)
INFO - root - 2020-04-09 16:07:05.797900: step 1570, total loss = 0.25, predict loss = 0.01 (5.4 examples/sec; 0.738 sec/batch; 0h:11m:26s remains)
INFO - root - 2020-04-09 16:07:13.586084: step 1580, total loss = 0.13, predict loss = 0.01 (5.6 examples/sec; 0.720 sec/batch; 0h:11m:02s remains)
INFO - root - 2020-04-09 16:07:21.312135: step 1590, total loss = 0.41, predict loss = 0.06 (5.6 examples/sec; 0.718 sec/batch; 0h:10m:53s remains)
INFO - root - 2020-04-09 16:07:39.894388: step 1600, total loss = 0.09, predict loss = 0.00 (5.6 examples/sec; 0.721 sec/batch; 0h:10m:48s remains)
INFO - root - 2020-04-09 16:07:47.777193: step 1610, total loss = 0.39, predict loss = 0.03 (5.6 examples/sec; 0.720 sec/batch; 0h:10m:40s remains)
INFO - root - 2020-04-09 16:08:06.354733: step 1620, total loss = 0.26, predict loss = 0.02 (5.6 examples/sec; 0.720 sec/batch; 0h:10m:33s remains)
INFO - root - 2020-04-09 16:08:14.123501: step 1630, total loss = 0.71, predict loss = 0.01 (5.6 examples/sec; 0.720 sec/batch; 0h:10m:26s remains)
INFO - root - 2020-04-09 16:08:31.821102: step 1640, total loss = 0.91, predict loss = 0.12 (5.5 examples/sec; 0.733 sec/batch; 0h:10m:30s remains)
INFO - root - 2020-04-09 16:08:39.628353: step 1650, total loss = 1.54, predict loss = 0.08 (5.4 examples/sec; 0.742 sec/batch; 0h:10m:30s remains)
INFO - root - 2020-04-09 16:08:58.253983: step 1660, total loss = 0.30, predict loss = 0.06 (5.6 examples/sec; 0.720 sec/batch; 0h:10m:05s remains)
INFO - root - 2020-04-09 16:09:06.938233: step 1670, total loss = 1.02, predict loss = 0.04 (5.5 examples/sec; 0.727 sec/batch; 0h:10m:03s remains)
INFO - root - 2020-04-09 16:09:14.702260: step 1680, total loss = 0.45, predict loss = 0.01 (5.5 examples/sec; 0.724 sec/batch; 0h:09m:53s remains)
INFO - root - 2020-04-09 16:09:32.467598: step 1690, total loss = 1.79, predict loss = 0.07 (5.6 examples/sec; 0.720 sec/batch; 0h:09m:43s remains)
INFO - root - 2020-04-09 16:09:40.190271: step 1700, total loss = 0.62, predict loss = 0.05 (5.6 examples/sec; 0.721 sec/batch; 0h:09m:36s remains)
INFO - root - 2020-04-09 16:09:58.274178: step 1710, total loss = 0.29, predict loss = 0.00 (5.6 examples/sec; 0.718 sec/batch; 0h:09m:27s remains)
INFO - root - 2020-04-09 16:10:06.954775: step 1720, total loss = 0.67, predict loss = 0.23 (5.5 examples/sec; 0.729 sec/batch; 0h:09m:28s remains)
INFO - root - 2020-04-09 16:10:25.018151: step 1730, total loss = 0.64, predict loss = 0.03 (5.6 examples/sec; 0.720 sec/batch; 0h:09m:14s remains)
INFO - root - 2020-04-09 16:10:32.822404: step 1740, total loss = 0.19, predict loss = 0.02 (5.6 examples/sec; 0.718 sec/batch; 0h:09m:05s remains)
INFO - root - 2020-04-09 16:10:51.591497: step 1750, total loss = 0.14, predict loss = 0.01 (0.3 examples/sec; 11.732 sec/batch; 2h:26m:39s remains)
INFO - root - 2020-04-09 16:10:59.459110: step 1760, total loss = 0.59, predict loss = 0.03 (5.5 examples/sec; 0.730 sec/batch; 0h:09m:00s remains)
INFO - root - 2020-04-09 16:11:08.074759: step 1770, total loss = 0.37, predict loss = 0.01 (5.6 examples/sec; 0.720 sec/batch; 0h:08m:45s remains)
INFO - root - 2020-04-09 16:11:26.083996: step 1780, total loss = 0.44, predict loss = 0.17 (5.5 examples/sec; 0.723 sec/batch; 0h:08m:40s remains)
INFO - root - 2020-04-09 16:11:33.819670: step 1790, total loss = 0.16, predict loss = 0.01 (5.5 examples/sec; 0.722 sec/batch; 0h:08m:32s remains)
INFO - root - 2020-04-09 16:11:51.901609: step 1800, total loss = 0.14, predict loss = 0.01 (5.6 examples/sec; 0.720 sec/batch; 0h:08m:23s remains)
INFO - root - 2020-04-09 16:11:59.822856: step 1810, total loss = 1.38, predict loss = 0.12 (5.5 examples/sec; 0.728 sec/batch; 0h:08m:22s remains)
INFO - root - 2020-04-09 16:12:17.870371: step 1820, total loss = 0.16, predict loss = 0.01 (5.5 examples/sec; 0.731 sec/batch; 0h:08m:16s remains)
INFO - root - 2020-04-09 16:12:26.539074: step 1830, total loss = 0.51, predict loss = 0.07 (5.5 examples/sec; 0.729 sec/batch; 0h:08m:08s remains)
INFO - root - 2020-04-09 16:12:34.290334: step 1840, total loss = 0.17, predict loss = 0.02 (5.5 examples/sec; 0.732 sec/batch; 0h:08m:03s remains)
INFO - root - 2020-04-09 16:12:52.128747: step 1850, total loss = 0.17, predict loss = 0.01 (5.6 examples/sec; 0.720 sec/batch; 0h:07m:48s remains)
INFO - root - 2020-04-09 16:13:00.016077: step 1860, total loss = 0.23, predict loss = 0.01 (5.6 examples/sec; 0.718 sec/batch; 0h:07m:39s remains)
INFO - root - 2020-04-09 16:13:17.863186: step 1870, total loss = 1.24, predict loss = 0.02 (5.6 examples/sec; 0.719 sec/batch; 0h:07m:32s remains)
INFO - root - 2020-04-09 16:13:26.519390: step 1880, total loss = 0.21, predict loss = 0.01 (5.6 examples/sec; 0.719 sec/batch; 0h:07m:25s remains)
INFO - root - 2020-04-09 16:13:44.423797: step 1890, total loss = 0.34, predict loss = 0.03 (5.5 examples/sec; 0.734 sec/batch; 0h:07m:27s remains)
INFO - root - 2020-04-09 16:13:52.154612: step 1900, total loss = 0.40, predict loss = 0.02 (5.6 examples/sec; 0.719 sec/batch; 0h:07m:11s remains)
INFO - root - 2020-04-09 16:14:10.183948: step 1910, total loss = 0.18, predict loss = 0.01 (5.4 examples/sec; 0.735 sec/batch; 0h:07m:13s remains)
INFO - root - 2020-04-09 16:14:17.912614: step 1920, total loss = 0.23, predict loss = 0.01 (5.6 examples/sec; 0.720 sec/batch; 0h:06m:57s remains)
INFO - root - 2020-04-09 16:14:26.601590: step 1930, total loss = 0.12, predict loss = 0.01 (5.4 examples/sec; 0.734 sec/batch; 0h:06m:58s remains)
INFO - root - 2020-04-09 16:14:45.183521: step 1940, total loss = 0.30, predict loss = 0.09 (5.5 examples/sec; 0.730 sec/batch; 0h:06m:48s remains)
INFO - root - 2020-04-09 16:14:52.976745: step 1950, total loss = 0.33, predict loss = 0.01 (5.5 examples/sec; 0.733 sec/batch; 0h:06m:43s remains)
INFO - root - 2020-04-09 16:15:11.382911: step 1960, total loss = 0.46, predict loss = 0.08 (5.5 examples/sec; 0.729 sec/batch; 0h:06m:33s remains)
INFO - root - 2020-04-09 16:15:19.109057: step 1970, total loss = 0.11, predict loss = 0.01 (5.6 examples/sec; 0.719 sec/batch; 0h:06m:20s remains)
INFO - root - 2020-04-09 16:15:37.678196: step 1980, total loss = 0.44, predict loss = 0.02 (5.5 examples/sec; 0.731 sec/batch; 0h:06m:20s remains)
INFO - root - 2020-04-09 16:15:45.421031: step 1990, total loss = 0.26, predict loss = 0.01 (5.6 examples/sec; 0.720 sec/batch; 0h:06m:06s remains)
INFO - root - 2020-04-09 16:16:03.326995: step 2000, total loss = 0.09, predict loss = 0.00 (0.4 examples/sec; 10.868 sec/batch; 1h:30m:33s remains)
INFO - root - 2020-04-09 16:16:11.209861: step 2010, total loss = 0.10, predict loss = 0.01 (5.6 examples/sec; 0.720 sec/batch; 0h:05m:52s remains)
INFO - root - 2020-04-09 16:16:18.969975: step 2020, total loss = 0.24, predict loss = 0.01 (5.6 examples/sec; 0.719 sec/batch; 0h:05m:45s remains)
INFO - root - 2020-04-09 16:16:37.548561: step 2030, total loss = 0.24, predict loss = 0.01 (5.6 examples/sec; 0.719 sec/batch; 0h:05m:37s remains)
INFO - root - 2020-04-09 16:16:45.353403: step 2040, total loss = 0.13, predict loss = 0.02 (5.5 examples/sec; 0.729 sec/batch; 0h:05m:35s remains)
INFO - root - 2020-04-09 16:17:03.550501: step 2050, total loss = 0.12, predict loss = 0.01 (5.6 examples/sec; 0.719 sec/batch; 0h:05m:23s remains)
INFO - root - 2020-04-09 16:17:11.446423: step 2060, total loss = 0.12, predict loss = 0.02 (5.5 examples/sec; 0.728 sec/batch; 0h:05m:20s remains)
INFO - root - 2020-04-09 16:17:29.478585: step 2070, total loss = 0.36, predict loss = 0.07 (5.6 examples/sec; 0.719 sec/batch; 0h:05m:09s remains)
INFO - root - 2020-04-09 16:17:37.218588: step 2080, total loss = 0.17, predict loss = 0.01 (5.6 examples/sec; 0.720 sec/batch; 0h:05m:02s remains)
INFO - root - 2020-04-09 16:17:45.884707: step 2090, total loss = 0.16, predict loss = 0.01 (5.6 examples/sec; 0.720 sec/batch; 0h:04m:55s remains)
INFO - root - 2020-04-09 16:18:05.221349: step 2100, total loss = 0.16, predict loss = 0.05 (5.5 examples/sec; 0.721 sec/batch; 0h:04m:48s remains)
INFO - root - 2020-04-09 16:18:13.148406: step 2110, total loss = 0.33, predict loss = 0.08 (5.5 examples/sec; 0.723 sec/batch; 0h:04m:41s remains)
INFO - root - 2020-04-09 16:18:30.927669: step 2120, total loss = 0.52, predict loss = 0.06 (5.5 examples/sec; 0.728 sec/batch; 0h:04m:36s remains)
INFO - root - 2020-04-09 16:18:38.694056: step 2130, total loss = 0.34, predict loss = 0.02 (5.6 examples/sec; 0.717 sec/batch; 0h:04m:25s remains)
INFO - root - 2020-04-09 16:18:58.214354: step 2140, total loss = 0.07, predict loss = 0.00 (5.6 examples/sec; 0.720 sec/batch; 0h:04m:19s remains)
INFO - root - 2020-04-09 16:19:05.963156: step 2150, total loss = 0.26, predict loss = 0.01 (5.6 examples/sec; 0.719 sec/batch; 0h:04m:11s remains)
INFO - root - 2020-04-09 16:19:25.247796: step 2160, total loss = 0.45, predict loss = 0.09 (5.4 examples/sec; 0.737 sec/batch; 0h:04m:10s remains)
INFO - root - 2020-04-09 16:19:33.014579: step 2170, total loss = 0.23, predict loss = 0.01 (5.5 examples/sec; 0.731 sec/batch; 0h:04m:01s remains)
INFO - root - 2020-04-09 16:19:40.796081: step 2180, total loss = 0.10, predict loss = 0.01 (5.6 examples/sec; 0.720 sec/batch; 0h:03m:50s remains)
INFO - root - 2020-04-09 16:20:00.322340: step 2190, total loss = 0.25, predict loss = 0.07 (5.5 examples/sec; 0.732 sec/batch; 0h:03m:46s remains)
INFO - root - 2020-04-09 16:20:08.127617: step 2200, total loss = 0.09, predict loss = 0.01 (5.5 examples/sec; 0.729 sec/batch; 0h:03m:38s remains)
INFO - root - 2020-04-09 16:20:27.776059: step 2210, total loss = 0.11, predict loss = 0.01 (5.6 examples/sec; 0.719 sec/batch; 0h:03m:28s remains)
INFO - root - 2020-04-09 16:20:35.539167: step 2220, total loss = 0.21, predict loss = 0.06 (5.5 examples/sec; 0.732 sec/batch; 0h:03m:24s remains)
INFO - root - 2020-04-09 16:20:53.199893: step 2230, total loss = 0.17, predict loss = 0.01 (5.6 examples/sec; 0.720 sec/batch; 0h:03m:14s remains)
INFO - root - 2020-04-09 16:21:01.865121: step 2240, total loss = 0.23, predict loss = 0.02 (5.5 examples/sec; 0.723 sec/batch; 0h:03m:07s remains)
INFO - root - 2020-04-09 16:21:20.033604: step 2250, total loss = 0.16, predict loss = 0.01 (0.4 examples/sec; 11.136 sec/batch; 0h:46m:24s remains)
INFO - root - 2020-04-09 16:21:27.950097: step 2260, total loss = 0.39, predict loss = 0.06 (5.5 examples/sec; 0.729 sec/batch; 0h:02m:54s remains)
INFO - root - 2020-04-09 16:21:35.729412: step 2270, total loss = 0.40, predict loss = 0.09 (5.6 examples/sec; 0.719 sec/batch; 0h:02m:45s remains)
INFO - root - 2020-04-09 16:21:52.791073: step 2280, total loss = 0.43, predict loss = 0.01 (5.6 examples/sec; 0.719 sec/batch; 0h:02m:38s remains)
INFO - root - 2020-04-09 16:22:01.448493: step 2290, total loss = 0.09, predict loss = 0.01 (5.5 examples/sec; 0.723 sec/batch; 0h:02m:31s remains)
INFO - root - 2020-04-09 16:22:19.810269: step 2300, total loss = 0.25, predict loss = 0.01 (5.5 examples/sec; 0.730 sec/batch; 0h:02m:25s remains)
INFO - root - 2020-04-09 16:22:27.682254: step 2310, total loss = 0.27, predict loss = 0.02 (5.4 examples/sec; 0.736 sec/batch; 0h:02m:19s remains)
INFO - root - 2020-04-09 16:22:46.456075: step 2320, total loss = 0.50, predict loss = 0.01 (5.6 examples/sec; 0.720 sec/batch; 0h:02m:09s remains)
INFO - root - 2020-04-09 16:22:54.179061: step 2330, total loss = 0.42, predict loss = 0.02 (5.5 examples/sec; 0.721 sec/batch; 0h:02m:02s remains)
INFO - root - 2020-04-09 16:23:02.019571: step 2340, total loss = 0.11, predict loss = 0.01 (5.4 examples/sec; 0.737 sec/batch; 0h:01m:57s remains)
INFO - root - 2020-04-09 16:23:21.214941: step 2350, total loss = 0.10, predict loss = 0.01 (5.6 examples/sec; 0.719 sec/batch; 0h:01m:47s remains)
INFO - root - 2020-04-09 16:23:29.121303: step 2360, total loss = 0.08, predict loss = 0.01 (5.4 examples/sec; 0.734 sec/batch; 0h:01m:42s remains)
INFO - root - 2020-04-09 16:23:47.498534: step 2370, total loss = 0.14, predict loss = 0.01 (5.4 examples/sec; 0.734 sec/batch; 0h:01m:35s remains)
INFO - root - 2020-04-09 16:23:55.242224: step 2380, total loss = 0.08, predict loss = 0.00 (5.5 examples/sec; 0.732 sec/batch; 0h:01m:27s remains)
INFO - root - 2020-04-09 16:24:14.081335: step 2390, total loss = 0.07, predict loss = 0.00 (5.5 examples/sec; 0.721 sec/batch; 0h:01m:19s remains)
INFO - root - 2020-04-09 16:24:22.776536: step 2400, total loss = 0.16, predict loss = 0.05 (5.3 examples/sec; 0.757 sec/batch; 0h:01m:15s remains)
INFO - root - 2020-04-09 16:24:41.788432: step 2410, total loss = 0.16, predict loss = 0.01 (5.5 examples/sec; 0.732 sec/batch; 0h:01m:05s remains)
INFO - root - 2020-04-09 16:24:49.539320: step 2420, total loss = 0.10, predict loss = 0.00 (5.4 examples/sec; 0.735 sec/batch; 0h:00m:58s remains)
INFO - root - 2020-04-09 16:24:57.309217: step 2430, total loss = 0.22, predict loss = 0.02 (5.5 examples/sec; 0.729 sec/batch; 0h:00m:51s remains)
INFO - root - 2020-04-09 16:25:15.322135: step 2440, total loss = 0.45, predict loss = 0.08 (5.6 examples/sec; 0.720 sec/batch; 0h:00m:43s remains)
INFO - root - 2020-04-09 16:25:24.002525: step 2450, total loss = 0.45, predict loss = 0.05 (5.5 examples/sec; 0.721 sec/batch; 0h:00m:36s remains)
INFO - root - 2020-04-09 16:25:42.566528: step 2460, total loss = 0.05, predict loss = 0.00 (5.5 examples/sec; 0.722 sec/batch; 0h:00m:28s remains)
INFO - root - 2020-04-09 16:25:50.354396: step 2470, total loss = 0.21, predict loss = 0.01 (5.5 examples/sec; 0.731 sec/batch; 0h:00m:21s remains)
INFO - root - 2020-04-09 16:26:08.200381: step 2480, total loss = 0.16, predict loss = 0.01 (5.6 examples/sec; 0.720 sec/batch; 0h:00m:14s remains)
INFO - root - 2020-04-09 16:26:15.964091: step 2490, total loss = 0.08, predict loss = 0.01 (5.6 examples/sec; 0.719 sec/batch; 0h:00m:07s remains)
INFO - bisenet-v2 - Completed after 0:53:03
